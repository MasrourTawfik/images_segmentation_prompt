Pipeline
=============


.. _transformer_architecture:

1. Transformer Architecture Explained
-------------------------------------

.. figure:: /Documentation/images/arch1.png
   :width: 400
   :align: center
   :alt: Alternative Text

The Transformer is a groundbreaking architecture in the field of natural language processing. In this context, we will explain the various aspects of this architecture.

    * **Introduction (Attention is All You Need)**

       <https://arxiv.org/pdf/1706.03762.pdf>

      This introduction highlights the basics of the Transformer, as described in the paper "Attention is All You Need".

    * **Tokenization**

      Tokenization is the process of converting text into tokens, the basic units on which the model operates.

    * **Embedding**

      Embedding transforms tokens into dense vectors, which represent words numerically.

    * **Positional encoding**

      Positional encoding adds information about the order of words in the sequence.

    * **Transformer block**

      The Transformer block is the centerpiece of this architecture, comprising layers of attention and fully connected neural networks.

    * **Softmax**

      Softmax is an activation function used to compute probability scores on the model's output.

.. _visual_transformer:

2. Visual Transformer (ViT)
----------------------------

<https://arxiv.org/pdf/2010.11929v2.pdf>

Explain the functioning and usage of the Visual Transformer.

.. figure:: /Documentation/images/ViT.png
    :width: 400
    :align: center
    :alt: Alternative Text

.. _detection_transformer(DeTR):

3. Detection Transformer
-------------------------

https://arxiv.org/pdf/2005.12872.pdf

Explain the functioning and usage of the Detection Transformer (DeTR).

.. figure:: /Documentation/images/DTR.jpg
    :width: 400
    :align: center
    :alt: Alternative Text
